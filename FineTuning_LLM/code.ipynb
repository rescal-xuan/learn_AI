{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer,AutoModelForCausalLM,AutoModel\n",
    "import  torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Anconda\\envs\\llm\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"./model/gpt2_chinese\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"./model/gpt2_chinese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', 'world', '!']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_txt ='hello world!'\n",
    "\n",
    "tokenizer.tokenize(input_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 8701, 8572, 106, 102], 'token_type_ids': [0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(input_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 8701, 8572, 106, 102]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_txt=tokenizer.encode(input_txt)  # 编码输入文本\n",
    "en_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] hello world! [SEP]'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "de_txt=tokenizer.decode(en_txt)  # 解码编码文本\n",
    "de_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world!====>{'input_ids': [101, 8701, 8572, 106, 102], 'token_type_ids': [0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1]}\n",
      "hello world!====>['hello', 'world', '!']\n",
      "[1,2,3,4,5]====>{'input_ids': [101, 138, 122, 117, 123, 117, 124, 117, 125, 117, 126, 140, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "[1,2,3,4,5]====>['[', '1', ',', '2', ',', '3', ',', '4', ',', '5', ']']\n",
      "12345====>{'input_ids': [101, 9700, 102], 'token_type_ids': [0, 0, 0], 'attention_mask': [1, 1, 1]}\n",
      "12345====>['12345']\n",
      "0.123====>{'input_ids': [101, 121, 119, 8604, 102], 'token_type_ids': [0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1]}\n",
      "0.123====>['0', '.', '123']\n",
      "This is a  test demo====>{'input_ids': [101, 100, 8310, 143, 10060, 10828, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}\n",
      "This is a  test demo====>['[UNK]', 'is', 'a', 'test', 'demo']\n"
     ]
    }
   ],
   "source": [
    "txts =['hello world!', '[1,2,3,4,5]', \n",
    "       '12345', \n",
    "       '0.123', \n",
    "       'This is a  test demo']\n",
    "\n",
    "for i  in txts:\n",
    "    print(f'{i}====>{tokenizer(i)}')\n",
    "    print(f'{i}====>{tokenizer.tokenize(i)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  8701,  8572,   106,   102,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0],\n",
       "        [  101,   138,   122,   117,   123,   117,   124,   117,   125,   117,\n",
       "           126,   140,   102],\n",
       "        [  101,  9700,   102,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0],\n",
       "        [  101,   121,   119,  8604,   102,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0],\n",
       "        [  101,   100,  8310,   143, 10060, 10828,   102,     0,     0,     0,\n",
       "             0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "tokenizer(txts, max_length=2,padding='longest',return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  torch.utils.data import  Dataset,DataLoader\n",
    "\n",
    "class  MyDataset1(Dataset):\n",
    "    def __init__(self,data):\n",
    "        self.data=data\n",
    "    def __len__(self):\n",
    "        return  len(self.data)\n",
    "    def  __getitem__(self,index):\n",
    "        return  tokenizer(self.data[index],return_tensors='pt',padding='longest')['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  torch.utils.data import  Dataset,DataLoader\n",
    "\n",
    "class  MyDataset2(Dataset):\n",
    "    def __init__(self,data):\n",
    "        self.data=data\n",
    "    def __len__(self):\n",
    "        return  len(self.data)\n",
    "    def  __getitem__(self,index):\n",
    "        return  self.data[index]\n",
    "def collate_fn(batch):\n",
    "    tokenized =tokenizer(batch,return_tensors='pt',padding='longest')\n",
    "    return  dict(labels=tokenized['input_ids'],\n",
    "        input_ids =tokenized['input_ids'],\n",
    "                 attention_mask=tokenized['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 101, 8701, 8572,  106,  102]]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myDataset=MyDataset1(data=txts)\n",
    "trainloader=DataLoader(dataset=myDataset,batch_size=1)\n",
    "next(iter(trainloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'labels': tensor([[ 101, 8701, 8572,  106,  102]]),\n",
       " 'input_ids': tensor([[ 101, 8701, 8572,  106,  102]]),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myDataset=MyDataset2(data=txts)\n",
    "trainloader=DataLoader(dataset=myDataset,batch_size=1,collate_fn=collate_fn)\n",
    "next(iter(trainloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(21129, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2SdpaAttention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=21129, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1000])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1000, 768])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_input =torch.randint(0,21129,(1,1000))\n",
    "print(test_input.shape)\n",
    "model.transformer.wte(test_input).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Anconda\\envs\\llm\\lib\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1.4285, 'train_samples_per_second': 3.5, 'train_steps_per_second': 0.7, 'train_loss': 13.096346855163574, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "train_arags = TrainingArguments(\n",
    "    learning_rate=1e-4,\n",
    "    output_dir=\"./output\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_eval_batch_size=4,\n",
    "    do_eval=False,\n",
    "    evaluation_strategy='no',\n",
    "    optim='adafactor',\n",
    "    logging_steps=10,\n",
    "    report_to='none',\n",
    "\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=train_arags,\n",
    "    train_dataset=myDataset,\n",
    "    data_collator=collate_fn,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "trainer.train()\n",
    "trainer.save_model('output')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Anconda\\envs\\llm\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,032,192 || all params: 103,101,696 || trainable%: 1.0011\n"
     ]
    }
   ],
   "source": [
    "from  transformers import BitsAndBytesConfig\n",
    "from peft import get_peft_model, LoraConfig,prepare_model_for_kbit_training\n",
    "import torch\n",
    "from transformers import AutoTokenizer,AutoModelForCausalLM\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_path ='./model/gpt2_chinese'\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, quantization_config=bnb_config, \n",
    "                                             device_map=\"auto\")\n",
    "\n",
    "lora_config = LoraConfig(r=16,lora_alpha=32,target_modules=[\"c_proj\"],\n",
    "                        lora_dropout=0.05,\n",
    "                        bias=\"none\",\n",
    "                        task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 107, which is longer than the specified 100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "程不争发现不管是师叔，或者是师兄们没有必要的情况下都是沉默寡言，一句话也不说，各自盘膝而坐，闭目养神。\n",
      "“这到底是因为师叔在场，师兄们不敢放肆，还是......？”\n",
      "程不争发现不管是师叔，或者是师兄们没有必要的情况下都是沉默寡言，一句话也不说，各自盘膝而坐，闭目养神。\n",
      "“这到底是因为师叔在场，师兄们不敢放肆，还是......？”\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.vectorstores import Chroma\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore') \n",
    " \n",
    "filepath = 'data/novel.txt'\n",
    "raw_documents = TextLoader(filepath, encoding='utf8').load()\n",
    " \n",
    "# 按行分割块\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=20,\n",
    "    separator=\"\\n\",\n",
    "    length_function=len,\n",
    "    is_separator_regex=True,\n",
    ")\n",
    "documents = text_splitter.split_documents(raw_documents)\n",
    "# 加载本地 embedding 模型\n",
    "embedding = HuggingFaceEmbeddings(model_name='./model/hub/AI-ModelScope/bge-small-zh-v1___5')\n",
    "# 创建向量数据库\n",
    "db = Chroma.from_documents(documents, embedding, persist_directory=r\"./chroma/\")\n",
    "db.persist()  # 确保嵌入被写入磁盘\n",
    "'''\n",
    "如果已经创建好了，可以直接读取\n",
    "db = Chroma(persist_directory=persist_directory, embedding_function=embedding)\n",
    "'''\n",
    " \n",
    "# 直接传入文本\n",
    "query = \"程不争有些好笑的原因\"\n",
    "docs = db.similarity_search(query, k=3)\n",
    "# docs = db.similarity_search_with_score(query, k=3)  # 带分数的\n",
    "print(docs[0].page_content)\n",
    " \n",
    "# 传入向量去搜索\n",
    "embedding_vector = embedding.embed_query(query)\n",
    "docs = db.similarity_search_by_vector(embedding_vector, k=3)\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={}, page_content='诸天玉碟之主：程不争\\n'), Document(metadata={}, page_content='诸天玉碟之主：程不争\\n'), Document(metadata={}, page_content='诸天玉碟之主：程不争\\n'), Document(metadata={}, page_content='诸天玉碟之主：程不争\\n')]\n"
     ]
    }
   ],
   "source": [
    "from langchain.retrievers import TFIDFRetriever\n",
    "with open('data/novel.txt', encoding='utf8') as f:\n",
    "    lst = f.readlines()\n",
    "retriever = TFIDFRetriever.from_texts(lst)\n",
    "result = retriever.get_relevant_documents(\"程不争\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 107, which is longer than the specified 100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "程不争发现不管是师叔，或者是师兄们没有必要的情况下都是沉默寡言，一句话也不说，各自盘膝而坐，闭目养神。\n",
      "“这到底是因为师叔在场，师兄们不敢放肆，还是......？”\n",
      "程不争发现不管是师叔，或者是师兄们没有必要的情况下都是沉默寡言，一句话也不说，各自盘膝而坐，闭目养神。\n",
      "“这到底是因为师叔在场，师兄们不敢放肆，还是......？”\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.vectorstores import Chroma\n",
    " \n",
    " \n",
    "filepath = 'data/novel.txt'\n",
    "raw_documents = TextLoader(filepath, encoding='utf8').load()\n",
    " \n",
    "# 按行分割块\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=20,\n",
    "    separator=\"\\n\",\n",
    "    length_function=len,\n",
    "    is_separator_regex=True,\n",
    ")\n",
    "documents = text_splitter.split_documents(raw_documents)\n",
    "# 加载本地 embedding 模型\n",
    "embedding = HuggingFaceEmbeddings(model_name='./model/hub/AI-ModelScope/bge-small-zh-v1___5')\n",
    "# 创建向量数据库\n",
    "db = FAISS.from_documents(documents, embedding)\n",
    "# 保存\n",
    "db.save_local(\"./faiss_index\")\n",
    "'''\n",
    "如果已经创建好了，可以直接读取\n",
    "db = FAISS.load_local(\"./faiss_index\", embeddings)\n",
    "'''\n",
    " \n",
    "# 直接传入文本\n",
    "query = \"程不争有些好笑的原因\"\n",
    "docs = db.similarity_search(query, k=3)\n",
    "# docs = db.similarity_search_with_score(query, k=3)  # 带分数的\n",
    "print(docs[0].page_content)\n",
    " \n",
    "# 传入向量去搜索\n",
    "embedding_vector = embedding.embed_query(query)\n",
    "docs = db.similarity_search_by_vector(embedding_vector, k=3)\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x0000017E9D0EFA60>, search_type='mmr', search_kwargs={'k': 30})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = db.as_retriever(search_type=\"mmr\", search_kwargs={'k': 30})  # 构建检索器\n",
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db._co"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "\n",
    "    api_key=\"AIzaSyAND9Q0o9s3mRTYcFLLocFTzZExypVsEXE\",\n",
    "\n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content='人工智能（AI）并非单一技术，而是一组旨在模仿人类智能的技术和算法的集合。其工作方式取决于具体类型，但一般而言，AI 系统依靠以下几个关键要素：\\n\\n**1. 数据:** AI 系统依赖大量数据进行学习。这些数据可以是图像、文本、音频、视频或任何其他形式的数字信息。数据的质量和数量直接影响 AI 系统的性能。  数据越多，模型通常越准确和强大。\\n\\n**2. 算法:** 算法是 AI 系统的核心，它们是一组指令，告诉计算机如何处理数据并从中学习。不同类型的 AI 使用不同的算法，例如：\\n\\n* **机器学习 (ML):**  ML 算法允许计算机从数据中学习模式，而无需明确编程。  这包括：\\n    * **监督学习:**  算法使用标记数据（即已知输入和输出）进行训练，学习将输入映射到输出。例如，训练一个识别猫的图像的系统，需要提供大量标记为“猫”的图像。\\n    * **无监督学习:**  算法使用未标记的数据进行训练，学习数据中的内在结构和模式。例如，将客户群分成不同的细分市场。\\n    * **强化学习:**  算法通过与环境交互进行学习，并根据其行为获得奖励或惩罚。例如，训练一个玩游戏的 AI 系统。\\n\\n* **深度学习 (DL):**  深度学习是机器学习的一个子集，使用人工神经网络（ANN）进行学习。ANN 具有多个层，允许它们处理更复杂的数据和模式。例如，用于图像识别、自然语言处理和语音识别等任务。\\n\\n* **自然语言处理 (NLP):**  NLP 允许计算机理解、解释和生成人类语言。这包括语音识别、机器翻译和情感分析等任务。\\n\\n* **计算机视觉:**  计算机视觉允许计算机“看到”和解释图像和视频。这包括对象识别、图像分类和图像分割等任务。\\n\\n**3. 模型:**  算法处理数据后，会创建一个模型。该模型是一个数学表示，捕捉数据中的模式和关系。模型用于对新数据进行预测或做出决策。\\n\\n**4. 训练:**  训练是指使用数据来调整模型参数的过程，使其能够更好地预测或做出决策。这通常是一个迭代过程，需要多次调整模型参数，直到其达到预期的性能水平。\\n\\n**简而言之，AI 的工作流程大致如下：**\\n\\n1. **收集和准备数据:** 收集大量相关数据并进行清洗和预处理。\\n2. **选择算法:** 选择合适的算法来处理数据并创建模型。\\n3. **训练模型:** 使用数据来训练模型，调整模型参数使其能够准确地执行任务。\\n4. **评估模型:**  评估模型的性能，并根据需要进行调整。\\n5. **部署模型:** 将训练好的模型部署到实际应用中。\\n\\n\\n需要注意的是，目前大部分 AI 系统都是狭义人工智能 (Narrow AI)，它们只能在特定任务上表现出色，而无法像人类一样具有普遍的智能。  通用人工智能 (AGI) 仍然是一个研究目标。\\n', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "\n",
    "    model=\"gemini-1.5-flash\",\n",
    "    n=1,\n",
    "    messages=[\n",
    "\n",
    "        {\"role\": \"system\", \"content\": \"你是一个有帮助的助手。\"},\n",
    "\n",
    "        {\n",
    "\n",
    "            \"role\": \"user\",\n",
    "\n",
    "            \"content\": \"给我解释一下 AI 是如何工作的\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print (response.choices [0].message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.010790561325848103, 0.03776538744568825, 0.0071558100171387196, 0.018407689407467842, 0.0523773729801178, 0.03344248980283737, 0.05653980001807213, -0.02108611911535263, 0.035850170999765396, 0.006577409338206053, -0.048622410744428635, 0.03038099594414234, 0.02788544073700905, -0.05758237466216087, -0.010874307714402676, -0.08296309411525726, 0.00896658655256033, -0.009481456130743027, -0.07354788482189178, 0.016750676557421684, -0.004255956970155239, 0.009940783493220806, 0.012476769275963306, 0.006542762275785208, -0.0080301184207201, -0.03046625852584839, 0.003932633902877569, 0.009038825519382954, 0.001194568583741784, -0.022764796391129494, 0.00279880384914577, 0.05642271786928177, 0.03560832515358925, -0.007082000840455294, 0.03799967095255852, 0.02858397364616394, -0.01719231903553009, 0.07472323626279831, 0.023688290268182755, -0.12232840061187744, -0.05243860185146332, -0.0027913018129765987, -0.02130924165248871, 0.021113833412528038, -0.0174113679677248, 0.020022055134177208, 0.02666529454290867, 0.008026338182389736, -0.07210732996463776, 0.03080654889345169, 0.02032185159623623, -0.038461606949567795, -0.029667813330888748, 0.02107101120054722, -0.048595208674669266, 0.012965146452188492, -0.03305262327194214, -0.019531548023223877, 0.08607059717178345, 0.031809523701667786, -0.09515221416950226, 0.025045404210686684, -0.061858881264925, -0.03744138777256012, 0.007257566787302494, -0.009502404369413853, -0.028705185279250145, -0.022047009319067, -0.04186929017305374, -0.014036929234862328, 0.009485803544521332, 0.016301674768328667, -0.044994741678237915, -0.04001963138580322, 0.003102291142567992, -0.013224958442151546, -0.0012947404757142067, -0.03276975825428963, 0.014933011494576931, 0.04756515473127365, -0.06265918910503387, 0.03932344168424606, 0.04009755700826645, 0.07913912832736969, 0.0051198601722717285, 0.0053806365467607975, 0.0493595190346241, -0.05009850859642029, -0.08247499167919159, -0.0008069931645877659, 0.07709047198295593, -0.00838143564760685, -0.019510706886649132, -0.010018352419137955, 0.039672449231147766, -0.028217975050210953, -0.07585395127534866, -0.1362340897321701, 0.07591814547777176, 0.09382756799459457, -0.008690453134477139, 0.019439471885561943, 0.010210592299699783, -0.07577032595872879, 0.04861726611852646, 0.022610317915678024, -0.01907508634030819, -0.009042199701070786, -0.08233446627855301, 0.01644202508032322, -0.0426647774875164, -0.023533938452601433, 0.024845225736498833, 0.0022729728370904922, 0.01787528023123741, -0.004468485247343779, -0.026583682745695114, 0.023575056344270706, -0.024375615641474724, 0.004249707330018282, 0.017996331676840782, -0.03462690860033035, -0.0792599692940712, 0.031211616471409798, 0.026010841131210327, -0.027465377002954483, 0.006719882134348154, 0.0020857411436736584, 0.01761690340936184, 0.01746492087841034, 0.053649403154850006, -0.09719900786876678, 0.02726726420223713, 0.0041374205611646175, -0.026371505111455917, -0.020569978281855583, 0.01849859207868576, 0.03474188968539238, 0.004058477934449911, -0.02588554471731186, -0.022454923018813133, -0.05139715224504471, -0.07802660018205643, 0.026631709188222885, -0.028420334681868553, -0.03949905186891556, -0.028402356430888176, 0.04028543457388878, -0.02622988633811474, -0.015416868031024933, -0.08897228538990021, -0.006946973968297243, 0.00710170017555356, -0.03887337073683739, -0.02123122848570347, 0.01805717498064041, 0.0344419963657856, -0.04805963113903999, 0.03656536340713501, -0.022134752944111824, 0.00817906204611063, -0.04252343624830246, 0.009770439006388187, -0.01689697988331318, -0.0260008592158556, -0.015728693455457687, -0.012875941582024097, -0.06102177873253822, -0.008011510595679283, -0.03141343593597412, 0.04607613757252693, 0.019290471449494362, -0.05614999681711197, -0.0626102164387703, -0.010851754806935787, -0.04783433675765991, 0.01892949640750885, -0.037196069955825806, -0.029953237622976303, -0.016390573233366013, 0.03903897851705551, 0.01227223314344883, -0.030694490298628807, -0.025610066950321198, -0.041420549154281616, 0.022946758195757866, 0.008837624453008175, 0.02584686502814293, 0.008095208555459976, 0.0430380254983902, 0.00742639834061265, -0.01789223775267601, 0.06770210713148117, 0.004289376083761454, 0.008603089489042759, 0.01378733478486538, 0.03245336934924126, -0.01590084098279476, -0.010305567644536495, -0.011169102974236012, 0.02103716880083084, -0.044015295803546906, -0.003294517984613776, -0.010823548771440983, 0.021978741511702538, -0.0013411757536232471, -0.07058368623256683, -0.04888467118144035, 0.012891210615634918, -0.02317993901669979, 0.02376372180879116, -0.02273913472890854, -0.00690293125808239, -0.016848333179950714, -0.037606850266456604, 0.016563672572374344, 0.05340617150068283, 0.006045289803296328, 0.059975627809762955, -0.002571186050772667, -0.06861039996147156, -0.008420819416642189, 0.01377375703305006, 0.025352496653795242, 0.02028140425682068, -0.0305411908775568, -0.03251797333359718, -0.06120520085096359, -0.014559393748641014, -0.02170858532190323, -0.04539666697382927, -0.004923057742416859, -0.010451139882206917, -0.02956731803715229, 0.05434400588274002, 0.0014316325541585684, 0.0164483729749918, -0.0548328272998333, 0.0035998490639030933, 0.0025368465576320887, 0.019528821110725403, -0.07159475982189178, 0.06815468519926071, 0.02410038933157921, 0.045986633747816086, 0.011254469864070415, 0.021867210045456886, 0.009737428277730942, 0.05949988588690758, -0.04802311584353447, -0.024196499958634377, -0.019624751061201096, -0.015714231878519058, -0.026338843628764153, -0.032651204615831375, -0.023289354518055916, 0.017006704583764076, -0.05780372768640518, 0.013706867583096027, -0.034762755036354065, 0.0052970717661082745, 0.0176913570612669, -0.004919999744743109, -0.05247485637664795, -0.04030724614858627, -0.10258959978818893, -0.00859836395829916, 0.020211149007081985, 0.04474553093314171, -0.0526076965034008, 0.012720085680484772, -0.05081895366311073, -0.035338424146175385, 0.0023963050916790962, -0.030323723331093788, 0.011844326741993427, -0.02357429638504982, 0.007814189419150352, -0.013991127721965313, -0.058796484023332596, 0.07445056736469269, 0.0465429462492466, -0.026994947344064713, 0.006451377645134926, -0.011758445762097836, -0.02837166003882885, 0.035541072487831116, 0.011907235719263554, -0.0052772038616240025, 0.023883866146206856, 0.012279005721211433, 0.011675804853439331, 0.03490255028009415, -0.025815939530730247, 0.04386910796165466, 0.017611755058169365, 0.04958317428827286, 0.039289481937885284, 0.013265370391309261, 0.03610560670495033, 0.005503167863935232, 0.021461468189954758, -0.06844760477542877, 0.013246484100818634, 0.04152173548936844, -0.029613517224788666, -0.01957281492650509, -0.03483524173498154, -0.03825502097606659, -0.020166005939245224, 0.021265221759676933, 0.06129218265414238, -0.04163709282875061, 0.014791393652558327, 0.014529740437865257, 0.010896045714616776, -0.11543422192335129, -0.03360319882631302, -0.05589929223060608, -0.05811789259314537, -0.012332974001765251, 0.014787249267101288, -0.09898144751787186, 0.02594282664358616, -0.0005415517371147871, 0.008792607113718987, -0.03125834837555885, 0.04435426741838455, 0.00942982267588377, 0.000868803181219846, 0.06188846752047539, -0.015446916222572327, -0.07226631790399551, -0.044506099075078964, 0.017528332769870758, 0.039571281522512436, -0.051951903849840164, 0.0032469776924699545, -0.003773163538426161, 0.034786321222782135, -0.009412731043994427, 0.026751253753900528, 0.03839461877942085, 0.05171748623251915, 0.012119683437049389, -0.07094941288232803, 0.06583517789840698, -0.00445381086319685, -0.019545819610357285, -0.03581184521317482, -0.03221912309527397, 0.06569118797779083, -0.0051487949676811695, -0.010263553820550442, 0.019870826974511147, -0.02322455495595932, -0.017076067626476288, -0.005842494312673807, -0.00406220369040966, -0.028368519619107246, -0.00017507618758827448, 0.026795191690325737, -0.010769983753561974, 0.004666486289352179, -0.010733024217188358, 0.0007423970964737236, 0.06672339886426926, 0.02625289559364319, -0.03991442918777466, 0.02290494740009308, 0.019931890070438385, 0.012672273442149162, -0.026643624529242516, -0.00849990826100111, 0.007982770912349224, 0.011553523130714893, 0.01313185878098011, 0.016604015603661537, 0.03154205158352852, -0.08784219622612, -0.05190761759877205, -0.03165704384446144, -0.005474531091749668, 0.04079993814229965, 0.0038400550838559866, 0.04367854446172714, -0.05150669813156128, -0.024722052738070488, 0.019934827461838722, 0.03144526109099388, -0.04412183165550232, 0.018581928685307503, -0.020380990579724312, 0.03142813965678215, 0.029637156054377556, 0.02292359434068203, -0.03589651361107826, 0.034826334565877914, -0.011483040638267994, -0.0017426611157134175, 0.0034471682738512754, 0.02760867029428482, 0.07813266664743423, -0.02380296029150486, -0.038689058274030685, -0.038704123347997665, 0.045501913875341415, -0.0018269248539581895, 0.04165354371070862, 0.0010681857820600271, -0.03636687994003296, -0.04324778541922569, -0.001840935554355383, -0.01632324978709221, -0.05531185492873192, -0.03420332074165344, 0.003457342740148306, -0.01638178899884224, 0.0062800198793411255, -0.011392820626497269, 0.0007934024324640632, -0.01728118024766445, 0.07051656395196915, 0.006142304744571447, 0.0010320268338546157, -0.07756688445806503, -0.01725735329091549, -0.017777113243937492, 0.025545917451381683, 0.03787548840045929, 0.0203388798981905, 0.01669018343091011, -0.0009158957982435822, -0.05879148468375206, 0.026536772027611732, 0.043445173650979996, -0.020170288160443306, -0.04645369201898575, -0.03924264758825302, -0.008544772863388062, -0.059880416840314865, -0.0038524868432432413, 0.05935370549559593, 0.02478773333132267, -0.036441951990127563, 0.03433588519692421, 0.0608387216925621, -0.025580616667866707, 0.03542158752679825, -0.08336429297924042, -0.014501698315143585, -0.04314250126481056, 0.0010071329306811094, 0.01394415833055973, -0.041038889437913895, 0.05676395073533058, 0.03354688733816147, 0.03245168179273605, -0.014439379796385765, 0.024018868803977966, -0.03283117339015007, -0.014726938679814339, 0.020077310502529144, 0.052366703748703, 0.018686365336179733, -0.030141469091176987, -0.06950796395540237, 0.02743900567293167, -0.044662877917289734, -0.027372850105166435, 0.02613779716193676, 0.008398720063269138, 0.014098933897912502, 0.00506135867908597, 0.03247535601258278, -0.01905762031674385, -0.019495641812682152, 0.021818963810801506, 0.015862062573432922, 0.03693776950240135, 0.014613614417612553, 0.053203411400318146, 0.033699098974466324, 0.03218269348144531, 0.08045617491006851, -0.0598096065223217, -0.05742032080888748, -0.03999164700508118, 0.033143650740385056, 0.004230685066431761, 0.06087212264537811, 0.024544531479477882, 0.019837401807308197, 0.006001709029078484, -0.031885817646980286, -0.030693262815475464, 0.007243372034281492, 0.022703975439071655, -0.017832132056355476, 0.026842646300792694, 0.06014237552881241, 0.0007703949231654406, 0.01269793976098299, -0.030843039974570274, 0.03642132133245468, -0.008549673482775688, -0.006215762812644243, -0.05973336845636368, 0.04635118693113327, 0.03762282058596611, 0.05289364978671074, 0.012984080240130424, 0.06197143346071243, -0.00251937797293067, -0.04079684242606163, 0.006324432324618101, -0.06220356747508049, 0.05567621812224388, -0.0270114466547966, 0.04752492904663086, -0.03955288231372833, -0.023822743445634842, 0.011177146807312965, -0.0002126664185198024, -0.015335524454712868, -0.018749145790934563, -0.01252884790301323, 0.017657190561294556, -0.0018241048092022538, -0.04749300330877304, -0.029716921970248222, 0.05201757326722145, 0.013973613269627094, 0.05358352139592171, -0.005169920157641172, 0.06108623743057251, -0.00034020538441836834, 0.00204732408747077, 0.022032570093870163, 0.009585895575582981, 0.007635426241904497, 0.010191744193434715, -0.0016849159728735685, 0.039626624435186386, -0.004662736319005489, -0.03499935194849968, 0.02045140229165554, 0.03854590654373169, -0.01136835291981697, 0.11801902204751968, 0.027295632287859917, -0.03454121947288513, -0.06620489805936813, -0.022455863654613495, -0.036504313349723816, -0.027751805260777473, 0.02426913008093834, -0.010416404344141483, -0.03599536791443825, -0.023112168535590172, 0.024780815467238426, -0.04206731915473938, 0.04737454652786255, 0.010679048486053944, 0.045824650675058365, -0.012001056224107742, 0.010149226523935795, 0.02402861788868904, 0.010881604626774788, -0.02032933197915554, 0.03122020699083805, 0.045521169900894165, 0.0019749668426811695, 0.00021772073523607105, 0.049741219729185104, -0.0032710381783545017, -0.02781662903726101, 0.01512980554252863, 0.06713394075632095, -0.0158261526376009, 0.02157570794224739, 0.015458326786756516, 0.06363721191883087, 0.012749752029776573, -0.026469390839338303, 0.009748454205691814, 0.005470410920679569, -0.031643062829971313, 0.036677468568086624, 0.07205954939126968, -0.050301443785429, 0.0030132040847092867, 0.03684893995523453, -0.027988746762275696, 0.047528158873319626, 0.021805569529533386, -0.034064192324876785, 0.007725028786808252, 0.0008075462537817657, 0.005813617259263992, -0.014014477841556072, -0.027894562110304832, 0.0014065718278288841, -0.035173263400793076, -0.02289736084640026, -0.06290272623300552, -0.022646894678473473, -0.041230879724025726, 0.06479673087596893, 0.0555911622941494, -0.040556855499744415, 0.007456792518496513, -0.04380102455615997, -0.023841260001063347, -0.04093717783689499, 0.009272179566323757, 0.00582896126434207, 0.0016683698631823063, 0.08151273429393768, 0.020490366965532303, 0.0024195886217057705, 0.006366673391312361, -0.026029929518699646, 0.015564830042421818, -0.05851592868566513, 0.006237562745809555, -0.05297559127211571, 0.019300058484077454, -0.025418123230338097, -0.026909632608294487, -0.024857250973582268, -0.006227914243936539, 0.036833859980106354, 0.011286056600511074, -0.003237767145037651, 0.01138349436223507, -0.016271600499749184, 0.01307633239775896, -0.007788393180817366, -0.06915568560361862, 0.005114726722240448, 0.040353816002607346, 0.04037889093160629, 0.023325007408857346, -0.06489475816488266, 0.002100869780406356, 0.010500557720661163, -0.027305060997605324, -0.03512360528111458, 0.04678768664598465, -0.024756107479333878, 0.04770822823047638, -0.012653275392949581, 0.04199439659714699, -0.03723035007715225, 0.021079018712043762, -0.040204621851444244, -0.055284418165683746, 0.026441583409905434, -0.018350470811128616, -0.04611121490597725, -0.03347180783748627, 0.01706829108297825, -0.010559997521340847, -0.019669044762849808, -0.01892927847802639, -0.040444862097501755, 0.02622443437576294, 0.046169232577085495, -0.04466930776834488, -0.006864430382847786, 0.024980543181300163, -0.00559600442647934, 0.05515807121992111, -0.029814261943101883, -0.027459153905510902, 0.050755176693201065, -0.02899012714624405, -0.02816769666969776, 0.010095490142703056, -0.0017784872325137258, -0.03370315209031105, 0.02437799796462059, -0.0002741724601946771, -0.013288548216223717, -0.021402321755886078, 0.021412288770079613, -0.0025492662098258734, 0.03122108243405819, -0.012782253324985504, 0.01649075746536255, -0.008483967743813992, -0.030046774074435234, 0.014040936715900898, -0.020978521555662155, 0.0903482437133789, -0.026532525196671486, 0.036920785903930664, 0.053320810198783875, 0.0033778005745261908, -0.03507322445511818, -0.028353866189718246, -0.054508112370967865, 0.0011092553613707423, 0.033273614943027496, 0.013821336440742016, 0.031121917068958282, 0.011944682337343693, 0.018730593845248222, -0.014167198911309242, -0.017753787338733673, 0.030909964814782143, 0.062414612621068954, -0.01106966007500887, 0.0007920278003439307, -0.0031770209316164255, 0.010590525344014168, -0.015446615405380726, 0.028239797800779343, 0.0732949748635292, -0.0019514415180310607, -0.038540735840797424, 0.01942184753715992, 0.048093531280756, 0.06383849680423737, 0.03807168826460838, -0.08535566180944443, 0.010056694969534874, -0.03838665783405304, 0.025605836883187294, -0.0011507237795740366, 0.024316413328051567, 0.01178177259862423, 0.016260962933301926, 0.009767747484147549, -0.007292686961591244, 0.027287857607007027, -0.042929600924253464, -0.01323598064482212, -0.02688555233180523, 0.03531723842024803, 0.008915160782635212, -0.004012776538729668, -0.05119200795888901, -0.015749920159578323, 0.021718118339776993, 0.008989322930574417, -0.06974360346794128, 0.014973211102187634, -0.011397730559110641, -0.07624229043722153, 0.017569998279213905, 0.060562871396541595, -0.0028631435707211494, 0.05745212733745575, -0.022273819893598557, 0.04074757173657417, -0.02943744696676731, 0.03616159036755562, 0.031222470104694366, -0.010470662266016006, 0.04254848137497902, -0.015562483109533787, 0.027763474732637405, -0.07111009955406189, 0.021353261545300484, 0.031004874035716057, -0.022343460470438004]\n"
     ]
    }
   ],
   "source": [
    "response = client.embeddings.create (\n",
    "\n",
    "    input=\"您的文本字符串在这里\",\n",
    "    model=\"text-embedding-004\"\n",
    ")\n",
    "print (response.data [0].embedding)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
